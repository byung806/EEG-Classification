{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "769dc706",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2075be99-abce-42b0-b74c-e5bedc871f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn import metrics\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, train_test_split\n",
    "from typing import Annotated, Literal, TypeVar\n",
    "import mne\n",
    "import sys\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import pywt\n",
    "import pandas as pd\n",
    "\n",
    "mne.set_log_level('WARNING')\n",
    "mne.set_config('MNE_BROWSE_RAW_SIZE', '16,8')\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc6b366",
   "metadata": {},
   "source": [
    "# Define Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc26c41",
   "metadata": {},
   "source": [
    "## Get Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c640b78",
   "metadata": {},
   "source": [
    "#### Bonn University"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ff65b9-8ca8-4736-9430-c1395b61e2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_bonn(sets_to_read: list = ['A', 'B', 'C', 'D', 'E']):\n",
    "    \"\"\"\n",
    "    Returns data for all specified sets from the Bonn University dataset\n",
    "    Data returned is in a dictionary with set letters as keys and arrays of mne raw data\n",
    "\n",
    "    Sets A and B are from healthy patients, while Sets C, D, and E are from epileptic patients.\n",
    "\n",
    "    Sets C and D are seizure-free segments while set E is during a seizure.\n",
    "\n",
    "    Each set contains 100 single-channel EEG segments of 23.6-sec duration. The EEGs were recorded at a sampling rate of 173.61 Hz with 12-bit resolution over a 10 mV range.\n",
    "    \"\"\"\n",
    "    time_segments = 100\n",
    "    time_points = 4096\n",
    "    freq = 173.61\n",
    "\n",
    "    # maps from set_letter to set_letter_alternate (for the filenames)\n",
    "    sets_to_file_prefixes = {\n",
    "        'A': 'Z',\n",
    "        'B': 'O',\n",
    "        'C': 'N',\n",
    "        'D': 'F',\n",
    "        'E': 'S'\n",
    "    }\n",
    "\n",
    "    raws_all_categories = {}\n",
    "    for set in sets_to_read:\n",
    "        if set not in sets_to_file_prefixes:\n",
    "            continue\n",
    "\n",
    "        set_letter_alternate = sets_to_file_prefixes[set]\n",
    "\n",
    "        raws_one_set = []\n",
    "        for i in range(time_segments):\n",
    "            filename = f'data/bonn/SET {set}/{set_letter_alternate}{str(i+1).zfill(3)}.txt'\n",
    "            z = np.loadtxt(filename)\n",
    "\n",
    "            info = mne.create_info(\n",
    "                ch_names=1,\n",
    "                sfreq=freq,\n",
    "                ch_types='eeg',\n",
    "            )\n",
    "            raw = mne.io.RawArray(\n",
    "                np.reshape(z[:time_points], (1, time_points)), info)\n",
    "            raws_one_set.append(raw)\n",
    "\n",
    "        raws_all_categories[set] = raws_one_set\n",
    "\n",
    "    return raws_all_categories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5fffb5",
   "metadata": {},
   "source": [
    "#### AHEPA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09ebaf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_ahepa(disease: Literal['F', 'A'] = 'F'):\n",
    "    \"\"\"\n",
    "    https://openneuro.org/datasets/ds004504/versions/1.0.7\n",
    "    A dataset of EEG recordings from: Alzheimer's disease, Frontotemporal dementia and Healthy subjects\n",
    "    \"\"\"\n",
    "    raws_all_categories = {}\n",
    "    # dictionary from set to 3d numpy array (time_segments, channels, time_points)\n",
    "\n",
    "    participants = pd.read_csv('data/AHEPA/participants.tsv', sep='\\t')\n",
    "\n",
    "    for disorder in {disease, 'C'}:\n",
    "        DISORDER = participants[participants['Group'] == disorder]\n",
    "\n",
    "        raws_one_disorder = []\n",
    "        for participant_id in DISORDER['participant_id']:\n",
    "            # read derivatives folder for preprocessed data\n",
    "            filename = f'data/AHEPA/derivatives/{participant_id}/eeg/{participant_id}_task-eyesclosed_eeg.set'\n",
    "            raw = mne.io.read_raw_eeglab(filename, preload=True)\n",
    "            # raw.plot()\n",
    "            raws_one_disorder.append(raw)\n",
    "\n",
    "        raws_all_categories[disorder] = raws_one_disorder\n",
    "\n",
    "    return raws_all_categories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fd6442",
   "metadata": {},
   "source": [
    "## Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df9f1ce",
   "metadata": {},
   "source": [
    "#### Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d398ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_raws(raws_all_categories, notch_freq=50, low_freq=0.1, high_freq=64):\n",
    "    for set in raws_all_categories:\n",
    "        for i in range(len(raws_all_categories[set])):\n",
    "            # if i > 0: break\n",
    "            # raws_all_sets[set][i].copy().compute_psd().plot()\n",
    "\n",
    "            # raws_all_sets[set][i].copy().plot(duration=5, n_channels=15, scalings=500)\n",
    "            raws_all_categories[set][i] = raws_all_categories[set][i].notch_filter(\n",
    "                freqs=notch_freq)\n",
    "            raws_all_categories[set][i] = raws_all_categories[set][i].filter(\n",
    "                l_freq=low_freq, h_freq=high_freq)\n",
    "            # raws_all_sets[set][i].copy().compute_psd().plot();\n",
    "            # raws_all_sets[set][i].copy().plot(duration=5, n_channels=15, scalings=500);\n",
    "    return raws_all_categories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9e95db",
   "metadata": {},
   "source": [
    "#### DWT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3388799b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discrete_wavelet_transform(raws_all_categories, channel_approach: Literal['average', 'each'] = 'average'):\n",
    "    dwt_outputs_all_sets = {}\n",
    "\n",
    "    # Loop through each set we're classifying (ex: F, C or A, B, C, D, E)\n",
    "    for set in raws_all_categories:\n",
    "        dwt_outputs_all_sets[set] = []\n",
    "\n",
    "        # Loop through each recording (sub-0XX)/time segment/epoch\n",
    "        for epoch in raws_all_categories[set]:\n",
    "            epoch_array = epoch.get_data()\n",
    "\n",
    "            # Loop through each channel in every epoch\n",
    "            for epoch in epoch_array:\n",
    "                data = epoch\n",
    "\n",
    "                if channel_approach == 'average':\n",
    "                    data = np.mean(data, axis=0)\n",
    "                    data = np.reshape(data, (1, data.shape[0]))\n",
    "\n",
    "                dwt_outputs_one_epoch = []\n",
    "\n",
    "                # Loop through each channel in the data\n",
    "                for channel in data:\n",
    "                    # 1D multilevel DWT\n",
    "                    delta, theta, alpha, beta, gamma = pywt.wavedec(\n",
    "                        channel, wavelet='db4', level=4)\n",
    "                    \n",
    "                    # print(delta.shape, theta.shape, alpha.shape, beta.shape, gamma.shape)\n",
    "                    \n",
    "                    dwt_outputs_one_epoch.append([delta, theta, alpha, beta, gamma])\n",
    "                \n",
    "                dwt_outputs_all_sets[set].append(dwt_outputs_one_epoch)\n",
    "\n",
    "                # low frequencies => high time resolution, low freq resolution\n",
    "                # high frequences => low time resolution, high freq resolution\n",
    "\n",
    "                # print(cA4.shape)  # 0.1-4 Hz   delta\n",
    "                # print(cD4.shape)  # 4-8 Hz     theta\n",
    "                # print(cD3.shape)  # 8-16 Hz    alpha\n",
    "                # print(cD2.shape)  # 16-32 Hz   beta\n",
    "                # print(cD1.shape)  # 32-64 Hz   gamma\n",
    "    return dwt_outputs_all_sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3619cd89",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fd2927",
   "metadata": {},
   "source": [
    "#### Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102c8e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def variance(data):\n",
    "    output = np.var(data)\n",
    "    # print('variance_data: ', output)\n",
    "    return output\n",
    "\n",
    "\n",
    "def standard_deviation(data):\n",
    "    \"\"\"\n",
    "    measures the amount of variation of a set of values\n",
    "    \"\"\"\n",
    "    output = np.std(data)\n",
    "    # print('std_data: ', output)\n",
    "    return output\n",
    "\n",
    "\n",
    "def kurtosis(data):\n",
    "    \"\"\"\n",
    "    measure of the 'tailedness' of the data or outliers\n",
    "    \"\"\"\n",
    "    # fisher = True is default, which subtracts 3 from the final value\n",
    "    output = scipy.stats.kurtosis(data, fisher=False)\n",
    "    # print('kurtosis output: ', output)\n",
    "    return output\n",
    "\n",
    "\n",
    "def nn_shannon_entropy(data):\n",
    "    \"\"\"\n",
    "    measures the uncertainty of the data (or how surprising it is)\n",
    "    \"\"\"\n",
    "    # squared so log is never taken of a negative number in the data\n",
    "    squared = data**2\n",
    "    output = np.sum(squared * np.log(squared))\n",
    "    # print('entropy output: ', output)\n",
    "    return output\n",
    "\n",
    "\n",
    "def logarithmic_band_power(data):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    output = np.log(np.sum(data**2) / data.shape[0])\n",
    "    # print('logarithmic band power output: ', output)\n",
    "    return output\n",
    "\n",
    "\n",
    "def compute_features(data):  # -> shape (#, 5)\n",
    "    # print('data shape: ', data.shape)\n",
    "    return np.array([\n",
    "        logarithmic_band_power(data),\n",
    "        standard_deviation(data),\n",
    "        variance(data),\n",
    "        kurtosis(data),\n",
    "        nn_shannon_entropy(data)\n",
    "    ], dtype=np.float64).T\n",
    "\n",
    "\n",
    "BandType = Literal['delta', 'theta', 'alpha', 'beta', 'gamma']\n",
    "FeatureType = Literal['LBP', 'Std', 'Var', 'Kur', 'SE']\n",
    "\n",
    "\n",
    "def graph_feature_vectors(\n",
    "    dataset: Literal['BONN', 'AHEPA'],\n",
    "    feature_vectors_all_sets: dict[str, np.ndarray],\n",
    "    band_x: BandType,\n",
    "    band_y: BandType,\n",
    "    feature: FeatureType,\n",
    "\n",
    "    features,\n",
    "):  \n",
    "    bands = ['delta', 'theta', 'alpha', 'beta', 'gamma']\n",
    "\n",
    "    if dataset == 'AHEPA':\n",
    "        color_mapping = {\n",
    "            'F': 'blue',\n",
    "            'C': 'green',\n",
    "        }\n",
    "    else:\n",
    "        color_mapping = {\n",
    "            'A': 'green',\n",
    "            'B': 'green',\n",
    "            'C': 'blue',\n",
    "            'D': 'blue',\n",
    "            'E': 'blue',\n",
    "        }\n",
    "    \n",
    "    # Loop through each available set\n",
    "    for set in feature_vectors_all_sets:\n",
    "\n",
    "        # Loop through each epoch\n",
    "        for epoch in feature_vectors_all_sets[set]:  # epoch has shape (# channels, # bands, # features)\n",
    "\n",
    "            # Loop through each channel\n",
    "            for channel in epoch:\n",
    "                x = channel[bands.index(band_x)][features.index(feature)]\n",
    "                y = channel[bands.index(band_y)][features.index(feature)]\n",
    "\n",
    "                plt.scatter(x, y, color=color_mapping[set], s=4)\n",
    "\n",
    "    plt.title(f'{band_x} vs {band_y} {feature} for {dataset}')\n",
    "    plt.xlabel(f'{band_x} {feature}')\n",
    "    plt.ylabel(f'{band_y} {feature}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492db95a",
   "metadata": {},
   "source": [
    "#### Feature Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3753f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_vectors_all_sets(dwt_outputs_all_sets, features):\n",
    "    # TODO: ipmlement features\n",
    "    feature_vectors_all_sets: dict[str, np.ndarray] = {}\n",
    "\n",
    "    # Loop through each set we're classifying (ex: F, C or A, B, C, D, E)\n",
    "    for set in dwt_outputs_all_sets:\n",
    "        feature_vectors_all_sets[set] = []\n",
    "\n",
    "        # Loop through each recording (sub-0XX)/time segment\n",
    "        for recording in dwt_outputs_all_sets[set]:\n",
    "\n",
    "            feature_vectors_one_recording = []\n",
    "\n",
    "            # Loop through each channel in the data\n",
    "            for channel in recording:\n",
    "                feature_vectors_one_recording.append(np.vstack([compute_features(band) for band in channel]))\n",
    "            \n",
    "            feature_vectors_all_sets[set].append(np.array(feature_vectors_one_recording))\n",
    "\n",
    "        feature_vectors_all_sets[set] = np.array(feature_vectors_all_sets[set])\n",
    "\n",
    "    return feature_vectors_all_sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9dd2a2c",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d19a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold_cross_validation(X, y, k_folds=10):\n",
    "    \"\"\"\n",
    "    k-fold cross validation\n",
    "    inputs: X, y, k_folds\n",
    "    \"\"\"\n",
    "    # https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html\n",
    "    kf = KFold(n_splits=k_folds, random_state=1, shuffle=True)\n",
    "    kf.get_n_splits(X)\n",
    "    accuracy = 0\n",
    "    accuracies = []\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        accuracy_this_fold = knn(X_train, y_train, X_test, y_test, k=5)\n",
    "        accuracy += accuracy_this_fold\n",
    "        accuracies.append(accuracy_this_fold)\n",
    "    print('Accuracies: ', accuracies)\n",
    "    return accuracy / k_folds\n",
    "\n",
    "\n",
    "def knn(X_train, y_train, X_test, y_test, k):\n",
    "    \"\"\"\n",
    "    normal knn classifier\n",
    "    determines the best k value and returns the best accuracy\n",
    "    inputs: train test split\n",
    "    \"\"\"\n",
    "    # https://www.tutorialspoint.com/scikit_learn/scikit_learn_kneighbors_classifier.htm\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train, y_train)\n",
    "    y_pred = knn.predict(X_test)\n",
    "    # confusion_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    return metrics.accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ee6d41",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa9985b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ CONSTANTS ------------------\n",
    "# Which dataset to use\n",
    "dataset: Literal['AHEPA', 'BONN'] = 'AHEPA'\n",
    "disease: Literal['F', 'A'] = 'F'\n",
    "\n",
    "# Whether to use the average of all channels or each channel individually when computing DWT\n",
    "channel_approach: Literal['average', 'each'] = 'average'\n",
    "\n",
    "\n",
    "# ------------------ AHEPA ------------------\n",
    "if dataset == 'AHEPA':\n",
    "    raws_all_categories = get_data_ahepa(disease=disease)\n",
    "\n",
    "# ------------------ BONN ------------------\n",
    "if dataset == 'BONN':\n",
    "    raws_all_categories = get_data_bonn()\n",
    "    raws_all_categories = filter_raws(\n",
    "        raws_all_categories, notch_freq=50, low_freq=0.1, high_freq=64)\n",
    "\n",
    "# raws_all_categories['C'][0].plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa8cfaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segment the data into 50 second epochs, or don't segment at all for BONN\n",
    "\n",
    "if dataset == 'AHEPA':\n",
    "    segment_length = 50\n",
    "else:\n",
    "    segment_length = 23\n",
    "\n",
    "for set in raws_all_categories:\n",
    "    for i, raw in enumerate(raws_all_categories[set]):\n",
    "        epochs = mne.make_fixed_length_epochs(raw, duration=segment_length)\n",
    "        raws_all_categories[set][i] = epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bc81c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Discrete Wavelet Transform to the data\n",
    "\n",
    "dwt_outputs_all_sets = discrete_wavelet_transform(raws_all_categories, channel_approach=channel_approach)\n",
    "\n",
    "print(len(dwt_outputs_all_sets['C']))          # num recordings\n",
    "print(len(dwt_outputs_all_sets['C'][0]))       # num channels\n",
    "print(len(dwt_outputs_all_sets['C'][0][0]))    # num freq bands\n",
    "print(len(dwt_outputs_all_sets['C'][0][0][0])) # num data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2825cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['LBP', 'Std', 'Var', 'Kur', 'SE']\n",
    "\n",
    "feature_vectors_all_sets = get_feature_vectors_all_sets(dwt_outputs_all_sets)\n",
    "\n",
    "print(feature_vectors_all_sets['C'].shape)          # num recordings, num channels, num freq bands, num features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd08ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph features & their individual impact?\n",
    "\n",
    "for feature in features:\n",
    "    graph_feature_vectors(\n",
    "        dataset,\n",
    "        feature_vectors_all_sets,\n",
    "        band_x='gamma',\n",
    "        band_y='beta',\n",
    "        feature=feature,\n",
    "        features=features\n",
    "    )"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a6e21bde",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "print([f'{f}_{b}' for b in ['delta', 'theta', 'alpha', 'beta', 'gamma'] for f in ['LBP', 'Std', 'Var', 'Kur', 'SE']])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f1930b01",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# write feature_vectors_all_sets to csv file\n",
    "import csv\n",
    "for set in feature_vectors_all_sets:\n",
    "    for i, recording in enumerate(feature_vectors_all_sets[set]):\n",
    "        with open(f'feature_vectors_{channel_approach}/{set}/{set}_{i}_feature_vectors.csv', mode='w') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([f'{f}_{b}' for b in ['delta', 'theta', 'alpha', 'beta', 'gamma'] for f in ['LBP', 'Std', 'Var', 'Kur', 'SE']])\n",
    "            for channel in recording:\n",
    "                writer.writerow([feature for band in channel for feature in band])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b8a844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert feature_vectors_all_sets to workable shape for classification (# recordings, features)\n",
    "# (# recordings, # megafeatures) or (# recordings, # channels, # features)  (which one would be better?)\n",
    "\n",
    "for set in feature_vectors_all_sets:\n",
    "\n",
    "    epochs = []\n",
    "    for i, epoch in enumerate(feature_vectors_all_sets[set]):\n",
    "        epochs.append(epoch.flatten())\n",
    "\n",
    "    feature_vectors_all_sets[set] = np.array(epochs)\n",
    "\n",
    "print(feature_vectors_all_sets['C'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53364b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to pandas dataframe\n",
    "\n",
    "if dataset == 'AHEPA':\n",
    "    # ------------------ AHEPA ------------------\n",
    "    set_mapping = {\n",
    "        disease: 1,\n",
    "        'C': 0\n",
    "    }  # 1 for Alzheimer's, 0 for healthy\n",
    "\n",
    "    if channel_approach == 'each':\n",
    "        channels = ['Fp1', 'Fp2', 'F7', 'F3', 'Fz', 'F4', 'F8', 'T3', 'C3', 'Cz', 'C4', 'T4', 'T5', 'P3', 'Pz', 'P4', 'T6', 'O1', 'O2']\n",
    "    else:\n",
    "        channels = ['Avg']\n",
    "else:\n",
    "    # ------------------ BONN ------------------\n",
    "    set_mapping = {\n",
    "        'A': 0,\n",
    "        'B': 0,\n",
    "        'C': 1,\n",
    "        'D': 1,\n",
    "        'E': 1\n",
    "    }  # 1 for epileptic, 0 for healthy\n",
    "\n",
    "    channels = ['C']\n",
    "\n",
    "data = pd.DataFrame(np.vstack([feature_vectors_all_sets[set] for set in feature_vectors_all_sets]))\n",
    "data.columns = [f'{channel}_{band}_{feature}' for channel in channels for band in ['A4', 'D4', 'D3', 'D2', 'D1'] for feature in ['LBP', 'Std', 'Var', 'Kur', 'SE']]\n",
    "data['Set'] = sum([[set_mapping[set]] * len(feature_vectors_all_sets[set]) for set in feature_vectors_all_sets], [])\n",
    "\n",
    "X = data.drop('Set', axis=1)\n",
    "y = data['Set'].values\n",
    "\n",
    "print(X.shape, y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3d4ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_fold_accuracy = k_fold_cross_validation(X.values, y, k_folds=10)\n",
    "print('Average of k folds:', k_fold_accuracy)\n",
    "\n",
    "\n",
    "# FTD - multi-channel with all 5 features\n",
    "# Accuracies:  [0.7974683544303798, 0.7721518987341772, 0.7341772151898734, 0.7341772151898734, 0.759493670886076, 0.6962025316455697, 0.7974683544303798, 0.7848101265822784, 0.6962025316455697, 0.6923076923076923]\n",
    "# Average of k folds: 0.746445959104187\n",
    "\n",
    "# FTD - average channel with all 5 features\n",
    "# Accuracies:  [0.6329113924050633, 0.6329113924050633, 0.4936708860759494, 0.7215189873417721, 0.569620253164557, 0.569620253164557, 0.6329113924050633, 0.5949367088607594, 0.5569620253164557, 0.6923076923076923]\n",
    "# Average of k folds: 0.6097370983446933"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
