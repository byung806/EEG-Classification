{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "769dc706",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2075be99-abce-42b0-b74c-e5bedc871f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, train_test_split\n",
    "from typing import Annotated, Literal, TypeVar\n",
    "import mne\n",
    "import sys\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import pywt\n",
    "import pandas as pd\n",
    "\n",
    "mne.set_log_level('WARNING')\n",
    "mne.set_config('MNE_BROWSE_RAW_SIZE', '16,8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc6b366",
   "metadata": {},
   "source": [
    "# Define Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc26c41",
   "metadata": {},
   "source": [
    "## Get Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c640b78",
   "metadata": {},
   "source": [
    "#### Bonn University"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "09ff65b9-8ca8-4736-9430-c1395b61e2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_bonn(sets_to_read: list = ['A', 'B', 'C', 'D', 'E']):\n",
    "    \"\"\"\n",
    "    Returns data for all specified sets from the Bonn University dataset\n",
    "    Data returned is in a dictionary with set letters as keys and arrays of mne raw data\n",
    "\n",
    "    Sets A and B are from healthy patients, while Sets C, D, and E are from epileptic patients.\n",
    "\n",
    "    Sets C and D are seizure-free segments while set E is during a seizure.\n",
    "\n",
    "    Each set contains 100 single-channel EEG segments of 23.6-sec duration. The EEGs were recorded at a sampling rate of 173.61 Hz with 12-bit resolution over a 10 mV range.\n",
    "    \"\"\"\n",
    "    time_segments = 100\n",
    "    time_points = 4096\n",
    "    freq = 173.61\n",
    "\n",
    "    # maps from set_letter to set_letter_alternate (for the filenames)\n",
    "    sets_to_file_prefixes = {\n",
    "        'A': 'Z',\n",
    "        'B': 'O',\n",
    "        'C': 'N',\n",
    "        'D': 'F',\n",
    "        'E': 'S'\n",
    "    }\n",
    "\n",
    "    raws_all_categories = {}\n",
    "    for set in sets_to_read:\n",
    "        if set not in sets_to_file_prefixes:\n",
    "            continue\n",
    "\n",
    "        set_letter_alternate = sets_to_file_prefixes[set]\n",
    "\n",
    "        raws_one_set = []\n",
    "        for i in range(time_segments):\n",
    "            filename = f'data/bonn/SET {set}/{set_letter_alternate}{str(i+1).zfill(3)}.txt'\n",
    "            z = np.loadtxt(filename)\n",
    "\n",
    "            info = mne.create_info(\n",
    "                ch_names=1,\n",
    "                sfreq=freq,\n",
    "                ch_types='eeg',\n",
    "            )\n",
    "            raw = mne.io.RawArray(\n",
    "                np.reshape(z[:time_points], (1, time_points)), info)\n",
    "            raws_one_set.append(raw)\n",
    "\n",
    "        raws_all_categories[set] = raws_one_set\n",
    "\n",
    "    return raws_all_categories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5fffb5",
   "metadata": {},
   "source": [
    "#### AHEPA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b09ebaf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_ahepa():\n",
    "    \"\"\"\n",
    "    https://openneuro.org/datasets/ds004504/versions/1.0.7\n",
    "    A dataset of EEG recordings from: Alzheimer's disease, Frontotemporal dementia and Healthy subjects\n",
    "    \"\"\"\n",
    "    raws_all_categories = {}\n",
    "    # dictionary from set to 3d numpy array (time_segments, channels, time_points)\n",
    "\n",
    "    participants = pd.read_csv('data/AHEPA/participants.tsv', sep='\\t')\n",
    "\n",
    "    for disorder in {'F', 'C'}:\n",
    "        DISORDER = participants[participants['Group'] == disorder]\n",
    "\n",
    "        raws_one_disorder = []\n",
    "        for participant_id in DISORDER['participant_id']:\n",
    "            # read derivatives folder for preprocessed data\n",
    "            filename = f'data/AHEPA/derivatives/{participant_id}/eeg/{participant_id}_task-eyesclosed_eeg.set'\n",
    "            raw = mne.io.read_raw_eeglab(filename, preload=True)\n",
    "            # raw.plot()\n",
    "            raws_one_disorder.append(raw)\n",
    "\n",
    "        raws_all_categories[disorder] = raws_one_disorder\n",
    "\n",
    "    return raws_all_categories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fd6442",
   "metadata": {},
   "source": [
    "## Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df9f1ce",
   "metadata": {},
   "source": [
    "#### Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e0d398ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_raws(raws_all_categories, notch_freq=50, low_freq=0.1, high_freq=64):\n",
    "    for set in raws_all_categories:\n",
    "        for i in range(len(raws_all_categories[set])):\n",
    "            # if i > 0: break\n",
    "            # raws_all_sets[set][i].copy().compute_psd().plot()\n",
    "\n",
    "            # raws_all_sets[set][i].copy().plot(duration=5, n_channels=15, scalings=500)\n",
    "            raws_all_categories[set][i] = raws_all_categories[set][i].notch_filter(\n",
    "                freqs=notch_freq)\n",
    "            raws_all_categories[set][i] = raws_all_categories[set][i].filter(\n",
    "                l_freq=low_freq, h_freq=high_freq)\n",
    "            # raws_all_sets[set][i].copy().compute_psd().plot();\n",
    "            # raws_all_sets[set][i].copy().plot(duration=5, n_channels=15, scalings=500);\n",
    "    return raws_all_categories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9e95db",
   "metadata": {},
   "source": [
    "#### DWT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3388799b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discrete_wavelet_transform(raws_all_categories, channel_approach: Literal['average', 'each'] = 'average'):\n",
    "    dwt_outputs_all_sets = {}\n",
    "\n",
    "    # Loop through each set we're classifying (ex: F, C or A, B, C, D, E)\n",
    "    for set in raws_all_categories:\n",
    "        dwt_outputs_all_sets[set] = []\n",
    "\n",
    "        # Loop through each recording (sub-0XX)/time segment/epoch\n",
    "        for epoch in raws_all_categories[set]:\n",
    "            epoch_array = epoch.get_data()\n",
    "\n",
    "            # Loop through each channel in every epoch\n",
    "            for epoch in epoch_array:\n",
    "                data = epoch\n",
    "\n",
    "                if channel_approach == 'average':\n",
    "                    data = np.mean(data, axis=0)\n",
    "                    data = np.reshape(data, (1, data.shape[0]))\n",
    "\n",
    "                dwt_outputs_one_epoch = []\n",
    "\n",
    "                # Loop through each channel in the data\n",
    "                for channel in data:\n",
    "                    # 1D multilevel DWT\n",
    "                    delta, theta, alpha, beta, gamma = pywt.wavedec(\n",
    "                        channel, wavelet='db4', level=4)\n",
    "                    \n",
    "                    # print(delta.shape, theta.shape, alpha.shape, beta.shape, gamma.shape)\n",
    "                    \n",
    "                    dwt_outputs_one_epoch.append([delta, theta, alpha, beta, gamma])\n",
    "                \n",
    "                dwt_outputs_all_sets[set].append(dwt_outputs_one_epoch)\n",
    "\n",
    "                # low frequencies => high time resolution, low freq resolution\n",
    "                # high frequences => low time resolution, high freq resolution\n",
    "\n",
    "                # print(cA4.shape)  # 0.1-4 Hz   delta\n",
    "                # print(cD4.shape)  # 4-8 Hz     theta\n",
    "                # print(cD3.shape)  # 8-16 Hz    alpha\n",
    "                # print(cD2.shape)  # 16-32 Hz   beta\n",
    "                # print(cD1.shape)  # 32-64 Hz   gamma\n",
    "    return dwt_outputs_all_sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3619cd89",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fd2927",
   "metadata": {},
   "source": [
    "#### Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "102c8e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def variance(data):\n",
    "    output = np.var(data)\n",
    "    # print('variance_data: ', output)\n",
    "    return output\n",
    "\n",
    "\n",
    "def standard_deviation(data):\n",
    "    \"\"\"\n",
    "    measures the amount of variation of a set of values\n",
    "    \"\"\"\n",
    "    output = np.std(data)\n",
    "    # print('std_data: ', output)\n",
    "    return output\n",
    "\n",
    "\n",
    "def kurtosis(data):\n",
    "    \"\"\"\n",
    "    measure of the 'tailedness' of the data or outliers\n",
    "    \"\"\"\n",
    "    # fisher = True is default, which subtracts 3 from the final value\n",
    "    output = scipy.stats.kurtosis(data, fisher=False)\n",
    "    # print('kurtosis output: ', output)\n",
    "    return output\n",
    "\n",
    "\n",
    "def nn_shannon_entropy(data):\n",
    "    \"\"\"\n",
    "    measures the uncertainty of the data (or how surprising it is)\n",
    "    \"\"\"\n",
    "    # squared so log is never taken of a negative number in the data\n",
    "    squared = data**2\n",
    "    output = np.sum(squared * np.log(squared))\n",
    "    # print('entropy output: ', output)\n",
    "    return output\n",
    "\n",
    "\n",
    "def logarithmic_band_power(data):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    output = np.log(np.sum(data**2) / data.shape[0])\n",
    "    # print('logarithmic band power output: ', output)\n",
    "    return output\n",
    "\n",
    "\n",
    "def compute_features(data):  # -> shape (#, 5)\n",
    "    # print('data shape: ', data.shape)\n",
    "    return np.array([\n",
    "        logarithmic_band_power(data),\n",
    "        standard_deviation(data),\n",
    "        variance(data),\n",
    "        kurtosis(data),\n",
    "        nn_shannon_entropy(data)\n",
    "    ], dtype=np.float64).T\n",
    "\n",
    "\n",
    "BandType = Literal['A4', 'D4', 'D3', 'D2', 'D1']\n",
    "FeatureType = Literal['LBP', 'Std', 'Var', 'Kur', 'SE']\n",
    "\n",
    "\n",
    "def graph_feature_vectors(\n",
    "    feature_vectors_all_sets: dict[str, np.ndarray],\n",
    "    band_x: BandType,\n",
    "    band_y: BandType,\n",
    "    feature: FeatureType,\n",
    "    flatten_channels=False,\n",
    "    graph_individual_channels: bool = False\n",
    "):\n",
    "    def get_feature(\n",
    "        feature_vector: np.ndarray,\n",
    "        band: BandType,\n",
    "        feature: FeatureType\n",
    "    ) -> np.float64:\n",
    "\n",
    "        bands = ['A4', 'D4', 'D3', 'D2', 'D1']\n",
    "        features = ['LBP', 'Std', 'Var', 'Kur', 'SE']\n",
    "\n",
    "        row = bands.index(band)\n",
    "        col = features.index(feature)\n",
    "\n",
    "        return feature_vector[\n",
    "            # channel * len(bands) * len(features)\n",
    "            row * len(features)\n",
    "            + col\n",
    "        ]\n",
    "\n",
    "    channels = 1  # BONN ONLY\n",
    "\n",
    "    colors = ['blue', 'green', 'yellow', 'red', 'black']\n",
    "    # colors = ['blue', 'blue', 'green', 'green', 'green']\n",
    "\n",
    "    # Loop through each available set\n",
    "    for i, set in enumerate(feature_vectors_all_sets):\n",
    "        # shape (# tsegments, # channels, 25)\n",
    "        feature_vectors = feature_vectors_all_sets[set]\n",
    "\n",
    "        # Loop through each time segment\n",
    "        for feature_vector in feature_vectors:  # shape (# channels, 25)\n",
    "            if graph_individual_channels and not flatten_channels:\n",
    "                # Loop thru channels and only graph specific parts of each feature vector\n",
    "                for channel in range(channels):\n",
    "                    x = get_feature(\n",
    "                        feature_vector[channel], band=band_x, feature=feature)\n",
    "                    y = get_feature(\n",
    "                        feature_vector[channel], band=band_y, feature=feature)\n",
    "\n",
    "                    plt.scatter(x, y, color=colors[i], s=4)\n",
    "            else:\n",
    "                if flatten_channels:\n",
    "                    x = get_feature(\n",
    "                        feature_vector, band=band_x, feature=feature)\n",
    "                    y = get_feature(\n",
    "                        feature_vector, band=band_y, feature=feature)\n",
    "                else:\n",
    "                    x = get_feature(\n",
    "                        feature_vector[0], band=band_x, feature=feature)\n",
    "                    y = get_feature(\n",
    "                        feature_vector[0], band=band_y, feature=feature)\n",
    "                plt.scatter(x, y, color=colors[i], s=4)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492db95a",
   "metadata": {},
   "source": [
    "#### Feature Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "3a3753f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_vectors_all_sets(dwt_outputs_all_sets):\n",
    "    feature_vectors_all_sets: dict[str, np.ndarray] = {}\n",
    "\n",
    "    # Loop through each set we're classifying (ex: F, C or A, B, C, D, E)\n",
    "    for set in dwt_outputs_all_sets:\n",
    "        feature_vectors_all_sets[set] = []\n",
    "\n",
    "        # Loop through each recording (sub-0XX)/time segment\n",
    "        for recording in dwt_outputs_all_sets[set]:\n",
    "\n",
    "            feature_vectors_one_recording = []\n",
    "\n",
    "            # Loop through each channel in the data\n",
    "            for channel in recording:\n",
    "                feature_vectors_one_recording.append(np.vstack([compute_features(band) for band in channel]))\n",
    "            \n",
    "            feature_vectors_all_sets[set].append(np.array(feature_vectors_one_recording))\n",
    "\n",
    "        feature_vectors_all_sets[set] = np.array(feature_vectors_all_sets[set])\n",
    "\n",
    "    return feature_vectors_all_sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9dd2a2c",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "80d19a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold_cross_validation(X, y, k_folds=10):\n",
    "    \"\"\"\n",
    "    k-fold cross validation\n",
    "    inputs: X, y, k_folds\n",
    "    \"\"\"\n",
    "    # https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html\n",
    "    kf = KFold(n_splits=k_folds, random_state=1, shuffle=True)\n",
    "    kf.get_n_splits(X)\n",
    "    accuracy = 0\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        accuracy_this_fold = knn(X_train, y_train, X_test, y_test, k=5)\n",
    "        accuracy += accuracy_this_fold\n",
    "        print(accuracy_this_fold)\n",
    "    return accuracy / k_folds\n",
    "\n",
    "\n",
    "def knn(X_train, y_train, X_test, y_test, k):\n",
    "    \"\"\"\n",
    "    normal knn classifier\n",
    "    determines the best k value and returns the best accuracy\n",
    "    inputs: train test split\n",
    "    \"\"\"\n",
    "    # https://www.tutorialspoint.com/scikit_learn/scikit_learn_kneighbors_classifier.htm\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train, y_train)\n",
    "    y_pred = knn.predict(X_test)\n",
    "    # confusion_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    return metrics.accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ee6d41",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "aaa9985b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning) \n",
    "\n",
    "dataset: Literal['AHEPA', 'BONN'] = 'AHEPA'\n",
    "\n",
    "# ------------------ AHEPA ------------------\n",
    "if dataset == 'AHEPA':\n",
    "    raws_all_categories = get_data_ahepa()\n",
    "\n",
    "# ------------------ BONN ------------------\n",
    "if dataset == 'BONN':\n",
    "    raws_all_categories = get_data_bonn()\n",
    "    raws_all_categories = filter_raws(\n",
    "        raws_all_categories, notch_freq=50, low_freq=0.1, high_freq=64)\n",
    "\n",
    "# raws_all_categories['C'][0].plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "afa8cfaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_length = 50\n",
    "for set in raws_all_categories:\n",
    "    for i, raw in enumerate(raws_all_categories[set]):\n",
    "        epochs = mne.make_fixed_length_epochs(raw, duration=segment_length)\n",
    "        raws_all_categories[set][i] = epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "10bc81c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "468\n",
      "1\n",
      "5\n",
      "1569\n"
     ]
    }
   ],
   "source": [
    "channel_approach = 'average'\n",
    "\n",
    "dwt_outputs_all_sets = discrete_wavelet_transform(raws_all_categories, channel_approach=channel_approach)\n",
    "\n",
    "print(len(dwt_outputs_all_sets['C']))          # num recordings\n",
    "print(len(dwt_outputs_all_sets['C'][0]))       # num channels\n",
    "print(len(dwt_outputs_all_sets['C'][0][0]))    # num freq bands\n",
    "print(len(dwt_outputs_all_sets['C'][0][0][0])) # num data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e2825cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------\n",
      "(468, 1, 5, 5)\n"
     ]
    }
   ],
   "source": [
    "feature_vectors_all_sets = get_feature_vectors_all_sets(dwt_outputs_all_sets)\n",
    "\n",
    "print('------------------')\n",
    "print(feature_vectors_all_sets['C'].shape)          # num recordings, num channels, num freq bands, num features"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a6e21bde",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "print([f'{f}_{b}' for b in ['delta', 'theta', 'alpha', 'beta', 'gamma'] for f in ['LBP', 'Std', 'Var', 'Kur', 'SE']])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f1930b01",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# write feature_vectors_all_sets to csv file\n",
    "import csv\n",
    "for set in feature_vectors_all_sets:\n",
    "    for i, recording in enumerate(feature_vectors_all_sets[set]):\n",
    "        with open(f'feature_vectors_{channel_approach}/{set}/{set}_{i}_feature_vectors.csv', mode='w') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([f'{f}_{b}' for b in ['delta', 'theta', 'alpha', 'beta', 'gamma'] for f in ['LBP', 'Std', 'Var', 'Kur', 'SE']])\n",
    "            for channel in recording:\n",
    "                writer.writerow([feature for band in channel for feature in band])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "50b8a844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------\n",
      "(468, 25)\n"
     ]
    }
   ],
   "source": [
    "# (# recordings, # megafeatures) or (# recordings, # channels, # features)  (which one would be better?)\n",
    "# Convert feature_vectors_all_sets to workable shape (# recordings, features)\n",
    "\n",
    "for set in feature_vectors_all_sets:\n",
    "\n",
    "    epochs = []\n",
    "    for i, epoch in enumerate(feature_vectors_all_sets[set]):\n",
    "        epochs.append(epoch.flatten())\n",
    "\n",
    "    feature_vectors_all_sets[set] = np.array(epochs)\n",
    "\n",
    "print('------------------')\n",
    "print(feature_vectors_all_sets['C'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "53364b1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "(789, 25) (789,)\n"
     ]
    }
   ],
   "source": [
    "# ------------------ AHEPA ------------------\n",
    "set_mapping = {\n",
    "    'F': 1,\n",
    "    'C': 0\n",
    "}  # 1 for FTD, 0 for Control\n",
    "channels = ['Fp1', 'Fp2', 'F7', 'F3', 'Fz', 'F4', 'F8', 'T3', 'C3', 'Cz', 'C4', 'T4', 'T5', 'P3', 'Pz', 'P4', 'T6', 'O1', 'O2']\n",
    "print(len(channels))\n",
    "\n",
    "\n",
    "data = pd.DataFrame(np.vstack([feature_vectors_all_sets[set] for set in feature_vectors_all_sets]))\n",
    "data.columns = [f'{band}_{feature}' for band in ['A4', 'D4', 'D3', 'D2', 'D1'] for feature in ['LBP', 'Std', 'Var', 'Kur', 'SE']]\n",
    "data['Set'] = sum([[set_mapping[set]] * len(feature_vectors_all_sets[set]) for set in feature_vectors_all_sets], [])\n",
    "\n",
    "X = data.drop('Set', axis=1)\n",
    "y = data['Set'].values\n",
    "\n",
    "print(X.shape, y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "5c3d4ce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5063291139240507\n",
      "0.6835443037974683\n",
      "0.6962025316455697\n",
      "0.5949367088607594\n",
      "0.6075949367088608\n",
      "0.5949367088607594\n",
      "0.620253164556962\n",
      "0.7215189873417721\n",
      "0.5316455696202531\n",
      "0.6923076923076923\n",
      "0.624926971762415\n"
     ]
    }
   ],
   "source": [
    "k_fold_accuracy = k_fold_cross_validation(X.values, y, k_folds=10)\n",
    "print(k_fold_accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
