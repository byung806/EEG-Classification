{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2075be99-abce-42b0-b74c-e5bedc871f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "import scipy\n",
    "import pywt\n",
    "\n",
    "mne.set_log_level('WARNING')\n",
    "mne.set_config('MNE_BROWSE_RAW_SIZE','16,8')\n",
    "\n",
    "# Types\n",
    "from typing import Annotated, Literal, TypeVar\n",
    "\n",
    "DType = TypeVar(\"DType\", bound=np.generic)\n",
    "ArrayN = Annotated[npt.NDArray[DType], Literal['N']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc6b366",
   "metadata": {},
   "source": [
    "### Get Bonn University data\n",
    "Sets A and B are from healthy patients, while Sets C, D, and E are from epileptic patients.\n",
    "\n",
    "Sets C and D are seizure-free segments while set E is during a seizure.\n",
    "\n",
    "Each set contains 100 single-channel EEG segments of 23.6-sec duration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "09ff65b9-8ca8-4736-9430-c1395b61e2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(channels: int, time_points: int) -> dict[str, np.ndarray]:\n",
    "    data = dict()\n",
    "\n",
    "    # maps from set_letter to set_letter_alternate (for the filenames)\n",
    "    sets = {\n",
    "        'A': 'Z',\n",
    "        'B': 'O',\n",
    "        'C': 'N',\n",
    "        'D': 'F',\n",
    "        'E': 'S'\n",
    "    }\n",
    "\n",
    "    for set_letter in sets:\n",
    "        set_letter_alternate = sets[set_letter]\n",
    "\n",
    "        set_data = np.zeros((channels, time_points))\n",
    "        for i in range(channels):\n",
    "            filename = f'data/bonn/SET {set_letter}/{set_letter_alternate}{str(i+1).zfill(3)}.txt'\n",
    "            z = np.loadtxt(filename)\n",
    "            set_data[i] = z[:time_points]\n",
    "\n",
    "        data[set_letter] = set_data\n",
    "\n",
    "    return data\n",
    "\n",
    "channels, time_points = 100, 4096\n",
    "freq = 173.61\n",
    "\n",
    "set_letter = 'D'\n",
    "data = get_data(channels, time_points)\n",
    "\n",
    "info = mne.create_info(\n",
    "    ch_names=[f'c{i}' for i in range(channels)],\n",
    "    sfreq=freq,\n",
    "    ch_types='eeg',\n",
    ")\n",
    "raw = mne.io.RawArray(data[set_letter], info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fd6442",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "### Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0d398ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jn/90br8jjn68b1psqfzrd83ry40000gn/T/ipykernel_9418/4053739601.py:6: RuntimeWarning: filter_length (5731) is longer than the signal (4096), distortion is likely. Reduce filter length or filter a longer signal.\n",
      "  raw = raw.filter(l_freq=0.1, h_freq=50)\n"
     ]
    }
   ],
   "source": [
    "# raw.copy().compute_psd().plot()\n",
    "\n",
    "# raw.copy().plot(duration=5, n_channels=15, scalings=500)\n",
    "\n",
    "raw = raw.notch_filter(freqs=50)\n",
    "raw = raw.filter(l_freq=0.1, h_freq=50)\n",
    "\n",
    "# raw.copy().compute_psd().plot();\n",
    "# raw.copy().plot(duration=5, n_channels=15, scalings=500);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9e95db",
   "metadata": {},
   "source": [
    "### Segmentation into intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3388799b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: see if the thing is actually split into time inveral??\n",
    "\n",
    "total_seconds = time_points/freq\n",
    "interval_length = 5  # in seconds\n",
    "\n",
    "# create a list of numpy arrays, each containing the data of a single interval\n",
    "intervals = []\n",
    "t = 0\n",
    "while t < time_points/freq:\n",
    "    intervals.append(data[set_letter][:, int(t*freq):int((t+interval_length)*freq)])\n",
    "    t += interval_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3619cd89",
   "metadata": {},
   "source": [
    "# Feature Extraction\n",
    "### Define 1D multilevel DWT function for each time interval (using Daubechies 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "102c8e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discrete_wavelet_transform(interval_data):\n",
    "    # 1D multilevel DWT\n",
    "    cA4, cD4, cD3, cD2, cD1 = pywt.wavedec(interval_data, wavelet='db4', level=4)\n",
    "    # low frequencies => high time resolution, low freq resolution\n",
    "    # high frequences => low time resolution, high freq resolution\n",
    "    # print(cA4.shape)  # 0.1-4 Hz   60\n",
    "    # print(cD4.shape)  # 4-8 Hz     60\n",
    "    # print(cD3.shape)  # 8-15 Hz    114\n",
    "    # print(cD2.shape)  # 15-30 Hz   222\n",
    "    # print(cD1.shape)  # 30-60 Hz   437\n",
    "    return cA4, cD4, cD3, cD2, cD1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492db95a",
   "metadata": {},
   "source": [
    "### Define feature vector functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3a3753f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def row_to_col_transpose(array):\n",
    "    return np.atleast_2d(array).T\n",
    "\n",
    "def variance(data) -> ArrayN[np.float64]:\n",
    "    variance_data = np.var(data, axis=1)\n",
    "    return variance_data  # shape (# channels,)\n",
    "\n",
    "def standard_deviation(data) -> ArrayN[np.float64]:\n",
    "    std_data = np.std(data, axis=1)\n",
    "    return std_data\n",
    "\n",
    "def kurtosis(data) -> ArrayN[np.float64]:\n",
    "    # fisher = True is default, which subtracts 3 from the final value\n",
    "    return scipy.stats.kurtosis(data, axis=1, fisher=False)\n",
    "\n",
    "def nn_shannon_entropy(data) -> ArrayN[np.float64]:\n",
    "    # non normalized shannon entropy - a measure of how uncertain the data is (or how surprising it is?)\n",
    "    squared = data**2\n",
    "    output = np.sum(squared * np.log(squared), axis=1)   # paper squared this i assume to get rid of negatives\n",
    "    return output\n",
    "    \n",
    "    # output2 = np.sum(data * np.log(data), axis=1)\n",
    "    # return output2   # returns nan because there are negative numbers\n",
    "    return scipy.stats.entropy(data, axis=1)  # returns -inf\n",
    "\n",
    "def logarithmic_band_power(data) -> ArrayN[np.float64]:\n",
    "    n = data.shape[1]\n",
    "    return np.log(np.sum(data**2, axis=1) / n)\n",
    "\n",
    "def compute_features(data):\n",
    "    features = np.array([\n",
    "        logarithmic_band_power(data),\n",
    "        standard_deviation(data),\n",
    "        variance(data),\n",
    "        kurtosis(data),\n",
    "        nn_shannon_entropy(data)\n",
    "    ], dtype=np.float64)\n",
    "    return features.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04695502",
   "metadata": {},
   "source": [
    "### Compute feature vector for each segment for each channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9a7ecce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 25)\n"
     ]
    }
   ],
   "source": [
    "for i, interval_data in enumerate(intervals):\n",
    "    # interval_data shape = (100, 868)\n",
    "    if i > 0: break   # TODO debug: to test only 1 interval\n",
    "\n",
    "    # Perform discrete wavelet transform on data\n",
    "    dwt_outputs = discrete_wavelet_transform(interval_data=interval_data)\n",
    "\n",
    "    # 5 features are extracted (LBP, SD, var, kur, SE) from each dwt output, so 5 rows, len(dwt_outputs) cols\n",
    "    outputs = []\n",
    "    for i, dwt_output in enumerate(dwt_outputs):\n",
    "        # dwt_output shape could be any of (100, 60), (100, 114), (100, 222), (100, 437)\n",
    "        # TODO: question: since there are 5 outputs from the DWT and 5 features, would there be 25 numbers in a feature vector? assuming yes\n",
    "        # TODO: should the feature vectors be calculated separately for each channel? this would result in 100 * 25 features for each time segment. assuming yes\n",
    "\n",
    "        # Compute features for each DWT output: [LBP, Std, Var, Kur, SE]\n",
    "        features = compute_features(dwt_output)   # shape (# channels, 5 features)\n",
    "\n",
    "        outputs.append(features)\n",
    "    feature_vector = np.hstack(outputs)\n",
    "    # [LBP_A4, SD_A4, Var_A4, Kurt_A4, Ent_A4\n",
    "    # LBP_D4, SD_D4, Var_D4, Kurt_D4, Ent_D4,\n",
    "    # LBP_D3, SD_D3, Var_D3, Kurt_D3, Ent_D3,\n",
    "    # LBP_D2, SD_D2, Var_D2, Kurt_D2, Ent_D2,\n",
    "    # LBP_D1, SD_D1, Var_D1, Kurt_D1, Ent_D1]\n",
    "\n",
    "    print(feature_vector.shape)  # (100, 25) - represents all the feature vectors of a specific time interval"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
